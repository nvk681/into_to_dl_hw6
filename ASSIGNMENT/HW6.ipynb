{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Home Work 5\n",
    "\n",
    "Panther ID: 002615185</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Problem 6</strong></h1>\n",
    "\n",
    "<h2>Attention Is All You Need</h2>\n",
    "\n",
    "<h3><b>Introduction</b></h3>\n",
    "\n",
    "In sequence modeling and transduction challenges like language modeling and machine translation, recurrent neural networks, particularly long short-term memory and gated recurrent neural networks, have firmly established themselves as state-of-the-art techniques. The symbol positions of the input and output sequences are commonly factored in recurrent models. Sequential computation's fundamental limitation, on the other hand, persists. In different tasks, attention mechanisms have become an essential feature of appealing sequence modeling and transduction models, allowing for the modeling of dependencies regardless of their distance in the input or output sequences. However, except in a few circumstances, such attention techniques are always utilized in conjunction with a recurrent network. After only twelve hours of training on eight P100 GPUs, the Transformer allows for substantially higher parallelization and can achieve a new state-of-the-art translation quality.\n",
    "\n",
    "<h3><b>Background</b></h3>\n",
    "\n",
    "The Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as a fundamental building block to compute hidden representations in parallel for all input and output points, to decrease sequential computation. In these models, the number of operations required to connect signals from two random input or output sites grows linearly for ConvS2S and logarithmically for ByteNet as the distance between them grows. This makes learning dependencies between distant sites more difficult. Self-attention, also known as intra-attention, is an attention mechanism that connects distinct points of a single sequence to compute a representation of it.\n",
    "\n",
    "End-to-end memory networks work well on simple-language question answering and language modeling tasks because they use a recurrent attention mechanism rather than sequence-aligned repetition. The transformer is the first transduction model to calculate its input and output representations using only self-attention rather than sequence-aligned RNNs or convolution.\n",
    "\n",
    "<h3><b>Model Architecture</b></h3>\n",
    "\n",
    "An encoder-decoder structure is used in most competitive neural sequence transduction models. The encoder converts a sequence of symbol representations (x 1,..., x n) into a sequence of continuous representations (z =). (z 1, ..., z n ). The model is auto-regressive at each phase, using previously created symbols as extra input to create the next. As depicted in the left and right portions of The Transformer -model architecture, the Transformer uses layered self-attention and point-wise, completely connected layers for both the encoder and decoder.\n",
    "\n",
    "\n",
    "<h3><b>Encoder and Decoder Stacks</b></h3>\n",
    "\n",
    "An encoder-decoder structure is used in most competitive neural sequence transduction models. The encoder converts a sequence of symbol representations (x 1,..., x n) into a sequence of continuous representations (z =). (z 1, ..., z n ). The model is auto-regressive at each phase, using previously created symbols as extra input to create the next. As depicted in the left and right portions of The Transformer -model architecture, the Transformer uses layered self-attention and point-wise, completely connected layers for both the encoder and decoder.\n",
    "\n",
    "\n",
    "<h3><b>Attention</b></h3>\n",
    "The mapping of a query and a set of key-value pairs to output can be characterized as an attention function, where the query, keys, values, and output are all vectors. The result is a weighted sum of the values, with the weight allocated to each value determined by the query's compatibility function with the relevant key.\n",
    "\n",
    "\n",
    "<h3><b>Scaled Dot-Product Attention</b></h3>\n",
    "In three ways, Transformer employs multi-head attention: • The queries in \"encoder-decoder attention\" levels come from the previous decoder layer, whereas the memory keys and values come from the encoder's output. This allows every decoder position to attend to overall input sequence positions. In sequence-to-sequence models, this mirrors the standard encoder-decoder attention methods. • Self-attention layers are included in the encoder. To keep the auto-regressive characteristic, we need to block leftward information flow in the decoder.\n",
    "\n",
    "\n",
    "<h3><b>Multi-Head Attention</b></h3>\n",
    "We discovered linearly projecting the queries, keys, and values h times with separate. Learned linear projections to d k, d k, and d v dimensions were more advantageous than conducting a single attention function with d model -dimensional keys, values, and queries. These are concatenated and projected again, yielding the final values. Averaging prevents this with a single attention head. We use h = 8 parallel attention layers or heads where the projections are parameter matrices. The total computational cost is comparable to single-head attention with full dimensions when we employ the lower dimension of each head.\n",
    "\n",
    "\n",
    "<h3><b>Applications of Attention in our Model</b></h3>\n",
    "In three ways, the Transformer employs multi-head attention: \n",
    "• The queries in \"encoder-decoder attention\" levels come from the previous decoder layer, whereas the memory keys and values come from the encoder's output. This allows every decoder position to attend to overall input sequence positions. In sequence-to-sequence models, this mirrors the standard encoder-decoder attention methods. • Self-attention layers are included in the encoder. To keep the auto-regressive characteristic, we need to block leftward information flow in the decoder.\n",
    "\n",
    "<h3><b>Position-wise Feed-Forward Networks</b></h3>\n",
    "An ultimately linked feed-forward network is applied to each position individually and identically in each layer of our encoder, decoder, and attention sub-layers. This is made up of two linear transformations separated by a ReLU activation. While the linear transformations are the same in all positions, the parameters of distinct layers are used. This can also be described as two convolutions with a kernel size of 1. The input and output layers have a dimensionality of d model = 512, while the inner layer has a dimensionality of df f = 2048.\n",
    "\n",
    "\n",
    "<h3><b>Embeddings and Softmax</b></h3>\n",
    "We use learnt embeddings to convert the input and output tokens to vectors of the dimension d model, just like other sequence transduction models. We employ the standard learned linear transformation and softmax function to convert the decoder output to projected next-token probabilities. Our model's two embedding layers and the pre-softmax linear transformation share the same weight matrix. We multiply those weights by d model in the embedding layers.\n",
    "\n",
    "<h3><b>Positional Encoding</b></h3>\n",
    "\n",
    "Because our model lacks recurrence and convolution, we must provide information about the relative or absolute positions of the tokens in the sequence for the model to use the order of the sequence. We append \"positional encodings\" to the input embeddings at the encoder and decoder stacks' bottoms. There are numerously learned and fixed positional encodings to choose from. The sine and cosine functions of various frequencies are used in this work, where pos is the position, and I is the dimension. From 2 through 10000 • 2, the wavelengths form a geometric progression. We chose this function because it would help the model learn to pay attention to relative positions fast. P E pos+k can be described as a linear function of P Epos for any fixed offset k.\n",
    "\n",
    "\n",
    "<h3><b>Why Self-Attention</b></h3>\n",
    "\n",
    "We compare various aspects of self-attention layers to recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n) to another sequence of equal length (z 1,..., z n), with x I z I R d, such as a hidden layer in a typical sequence transduction encoder or decoder, in this section. We discuss three desiderata to motivate our use of self-attention. As a result, we compare the maximum path length between any two input and output places in networks made up of various layer types.\n",
    "\n",
    "Self-attention could be limited to only evaluating a neighborhood of size r in the input sequence centered around the relevant output position to increase computing performance for very long sequence applications. In future research, we intend to look into this approach further. However, even when k = n, the complexity of a separable convolution is the same as combining a self-attention layer with a point-wise feed-forward layer, as we do in our model.\n",
    "\n",
    "<h3><b>Training Data and Batching</b></h3>\n",
    "\n",
    "We used the WMT 2014 English-German dataset containing around 4.5 million sentence pairings. With a shared source-target vocabulary of around 37000 tokens, sentences were encoded using byte-pair encoding. We used the WMT 2014 English-French dataset, which had 36 million sentences and was separated into 32000 word-piece vocabulary for English-French. Sentence pairs were grouped based on the length of the sequence. Each training batch contains around 25000 source tokens and 25000 target tokens in a set of sentence pairs.\n",
    "\n",
    "<h3><b>Hardware and Schedule</b></h3>\n",
    "Our models were trained on a single system with eight NVIDIA P100 GPUs. Each training step took roughly 0.4 seconds for our base models utilizing the hyperparameters specified throughout the paper. The basis models were trained for 100,000 steps or 12 hours. The step time for our large models was 1.0 second. The huge models were put through 300,000 steps of training (3.5 days).\n",
    "\n",
    "<h3><b>Optimizer</b></h3>\n",
    "\n",
    "We used the Adam optimizer with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the learning rate throughout training according to the formula: This corresponds to increasing the learning rate linearly for the first warmup_steps training steps and decreasing it proportionally to the inverse square root of the step number.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Regularization</b></h3>\n",
    "\n",
    "During training, we use three different methods of regularization: Residual Dropout is a term that refers to a person who has We dropout each sub-output layer's before adding it to the sub-layer input and normalizing it. In both the encoder and decoder stacks, we also apply dropout to the sums of the embeddings and the positional encodings. We utilize a rate of P drop = 0.1 in the base model. Smoothing the labels We used label smoothing with a value of ls = 0.1 during training. Perplexity suffers when the model learns to be more unsure, while accuracy and the BLEU score improve.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Machine Translation</b></h3>\n",
    "\n",
    "The huge transformer model surpasses the best previously reported models (including ensembles) by more than 2.0 BLEU on the WMT 2014 English-to-German translation problem, setting a new state-of-the-art BLEU score of 28.4. Our huge model outperformed all previously published single models on the WMT 2014 English-to-French translation test, achieving a BLEU score of 41.0 at less than 1/4 the training cost of the previous state-of-the-art model on the WMT 2014 English-to-French translation job. We averaged the last 5 checkpoints written at 10-minute intervals to create a single model for the basic models. With a beam size of 4 and a length penalty of 0.6, we used a beam search. After testing on the development set, these hyperparameters were chosen.\n",
    "\n",
    "\n",
    "<h3><b>Model Variations</b></h3>\n",
    "\n",
    "We modified our base model in numerous ways to evaluate the importance of different Transformer components, measuring the change in performance on the development set, newstest2013, on English-to-German translation. As mentioned in the previous section, we employed beam search but no checkpoint averaging. While single-head attention is 0.9 BLEU less effective than the optimal option, quality suffers as the number of heads increases. Reducing the attention key size d k hurts model quality, as shown in Table 3 rows (B). This implies that determining compatibility is difficult and that a more comprehensive compatibility function than the dot product would be advantageous.\n",
    "\n",
    "\n",
    "<h3><b>English Constituency Parsing</b></h3>\n",
    "\n",
    "We tested the Transformer on English constituency parsing to see if it might be applied to other tasks. The output is subject to solid structural limitations and is substantially longer than the input, posing unique issues. Furthermore, RNN sequence-to-sequence models have not yet achieved state-of-the-art performances in small-data regimes. We raised the maximum output length to input length + 300 during inference. Despite the lack of task-specific tweaking, our findings reveal that our model outperforms all previous models except the Recurrent Neural Network Grammar, delivering better outcomes. The Transformer beats the Berkeley-Parser even when trained simply on the WSJ training set of 40K phrases, in contrast to RNN sequence-to-sequence models.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Conclusion</b></h3>\n",
    "\n",
    "We developed the Transformer, the first sequence transduction model based purely on attention, in which multi-headed self-attention replaces the recurrent layers most typically used in encoder-decoder designs. We complete fresh WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks that are state-of-the-art. In the former challenge, our top model surpasses all previously published ensembles. We are optimistic about the future of attention-based models and intend to use them in various situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
