{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Home Work 5\n",
    "\n",
    "Panther ID: 002615185</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Problem 1:</b><h1>\n",
    "\n",
    "<h2><b>a)</b></h2>\n",
    "\n",
    "Steps to build one-hot encoding  \n",
    "\n",
    "Make a list of every word in the corpus. \n",
    "\n",
    "Determine whether a specific word is present or absent in a review. The presence of one is represented by one, and the absence by zero. After then, each review will be represented as a tuple of zero and one element. \n",
    "\n",
    "All of the documents' one-hot encoding representations can be generated.\n",
    "\n",
    "\n",
    "This will give us a vector of lenght same as the size of the vocabulary.\n",
    "\n",
    "\n",
    "<h2><b>b)</b></h2>\n",
    "Because real-world vocabulary is typically large, the size of the vector representing each document will be significant as well, regardless of the number of words in the text. \n",
    "\n",
    "EX: 171,476 words are in english as of now. This will create every document to have like 171,476 dimensions. \n",
    "\n",
    "\n",
    "The order in which the words occur in the review/document is completely lost, which sadly results in a loss of context. \n",
    "\n",
    "EX: \"Man bites dog\" and \"Dog bites man\". These have the same exact words, but they mean something that is opposite to each other. \n",
    "\n",
    "Because of the binary encoding, the frequency information of words is lost. For example, the words tremendous and better appear twice in the first review, but there is no way to describe that; all we know is that they exist.\n",
    "\n",
    "EX: \"Fear leads to anger; anger leads to hatred; hatred leads to conflict; conflict leads to suffering.\", as you can see same word is used multiple times, and those words need to exist multiple times to make the sentence work. When this is one-hot encoding it will totall be lost.\n",
    "\n",
    "\n",
    "\n",
    "<h2><b>c)</b></h2>\n",
    "Options to fix these issues can be. \n",
    "\n",
    "<h3><b>1)Count Vectors: </b></h3>\n",
    "\n",
    "This approach is similar to on-hot encoding, but it has the added benefit of recognizing the frequency/counts of words in the documents they appear. The size of the vector represents each document. However, a typical technique for addressing this problem is to select only the top n terms depending on their frequency. Each word's count is given by n. There was no attempt to capture the context of the words. They had lost their sense of semantics and word relationships.\n",
    "\n",
    "<h3><b>2)Tf-Idf: </b></h3>\n",
    "\n",
    "This approach is a step forward from count vectors and is commonly utilized in search technology. Term frequency-Inverse document frequency is abbreviated as Tf-Idf. \n",
    "\n",
    "The frequency with which the same word/term appears throughout the corpus D. df is a mathematical representation of this expression (Wi, D). \n",
    "Idf is a metric that assesses how rarely the term Wi appears in corpus D. \n",
    "Tf gives the terms that appear more frequently in a single document more weight. Idf, on the other hand, will attempt to down-weight words that appear several times across the corpus, such as \"the,\" \"this,\" \"a,\" \"an,\" and so on. Then combining them (Tf-Idf) aids in catching the document's unusual terms that do not appear frequently. \n",
    "In a document, it captures both the importance and the frequency of a term.\n",
    "Each word is still recorded separately.  The context in which it appears is not recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Problem 2:</b></h1>\n",
    "\n",
    "The 0 and 1 mean the decision's lower and upper bounds. The limits are based on the sigmoid activation function that is available. For example, if it is a classification problem, we can say 0 and 1 are the two different classes, and the relative value to any limit will fall into that class as the bias for the RNN is 0. \n",
    "\n",
    "\n",
    "\n",
    "The function the RNN is trying to calculate is a linear equation coupled with an activation function. \n",
    "\n",
    "The Weights are multiplied by the input values as X. Let us name the weight W. \n",
    "\n",
    "We add bias for each node which can be named B. \n",
    "\n",
    "Basic computation on the RNN, W*X+B\n",
    "\n",
    "The output of the function is processed through an activation function. Based on the preference, an activation function as sigmoid, and ReLu, lekyReLu.\n",
    "\n",
    "So the final output is activation(W*X+B). Based on the Given Activation function, we need to use logistic sigmoid. \n",
    "\n",
    "y = 1/(1+(e^(-(W * X+B))))\n",
    "\n",
    "Is the equation for the RNN to work. \n",
    "\n",
    "As we can see, a feedback loop exists; we need to use it for the calculation. \n",
    "\n",
    "Let us assume the feedback loop input is x_0. This is the state or output of the previous step in RNN. \n",
    "\n",
    "As we can let the RNN model learn about using the previous state, we will allow the weight W_0 to make the decision. \n",
    "\n",
    "The resulting linear equation for the RNN is as \n",
    "\n",
    "z = (W_0 * X_0)+(W * X)+B\n",
    "\n",
    "Given X_0 is the hidden state or the feedback loop. Here the result is determined weighted sum of current input and previous cell output. \n",
    "\n",
    "\n",
    "The result value with the activation function is. \n",
    "\n",
    "Y = 1/ ( 1 + (e^(-(  (W_0 * X_0)+(W * X)+B))))\n",
    "\n",
    "\n",
    "This is the result of the RNN, and the model can be trained to use the input and previous output to make decisions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Problem 3:</b></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Problem 4:</b></h1>\n",
    "\n",
    "Dropout cannot be applied to RNNs since it will impair the RNN's ability to maintain long-term dependency. \n",
    "\n",
    "Instead of operating on the RNN's hidden states, limitations on the recurrent matrices can be used to regularize the network. This can be accomplished simply by limiting the matrix's capacity, and it can be applied to existing LSTM implementations without requiring any changes. This weight-dropped LSTM uses a DropConnect mask on the hidden-to-hidden recurrent weights to apply recurrent regularisation.\n",
    "\n",
    "We can prevent overfitting on the LSTM's recurrent connections by running DropConnect on the hidden-to-hidden weight matrices within the LSTM. This regularization method would also prevent overfitting other RNN cells' recurrent weight matrices. \n",
    "The same individual dropped weights remain dropped for the whole forward and backward pass since the identical weights are repeated throughout several timesteps. The result is similar to variational dropout, which performs dropout on ht1 and applies the same dropout mask on recurrent connections within the LSTM, except that the dropout is applied to the recurrent weights. DropConnect might also be applied to the LSTM's non-recurrent weights.\n",
    "\n",
    "Weight Constraints: a viable alternative to weight decay is to impose a tight upper bound on weights l2-norm. \n",
    "\n",
    "Activity Constraints: do not bother; if you have to confine your outputs for most purposes manually, the layer is probably not learning well, and the solution lies elsewhere.\n",
    "\n",
    "\n",
    "Result: \n",
    "\n",
    "As we know that dropout, and other regulirization prevents the RNN from overfitting, The methods of regularization need to be updated based on RNN. The output, results and the theory for it will remain the same and helps us improve the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Problem 5:</b></h1>\n",
    "\n",
    "Teacher forcing is a recurrent neural network training approach that uses ground truth as input instead of model output from a previous step.\n",
    "\n",
    "Teacher forcing is a recurrent neural network training approach that uses ground truth as input instead of model output from a previous step. This helps the model learn and work on the next words based on the current error, rather than an loss at the end of the operation. \n",
    "\n",
    "\n",
    "Assume building an RNN to predict the next word for a text sequence. \"When Rivers Run Into The Ocean\" Is the sentence is the expected output. \n",
    "\n",
    "Without teacher forcing ratio: Here is a normal RNN; we calculate the loss and train the RNN; the RNN input will be the input of the final output as an input. When there is an output like \"the\" a lot of context will be lost, or the RNN will have significantly fewer details to work on to generate the following words. Moreover, the loss of the output can calculated wrong. This will through the RNN off and create an unstable RNN model.  \n",
    "\n",
    "Teacher forcing ratio: When w new word is being generated, in place of the last output as current input, we use the while string generated till now as the input. The model will have error to work on for every word the RNN generates. This helps model to be directed to the expected answer, rather than updating itself at the end of the output. This will give better content for RNN and help the model predict or generate the next word accurately. \n",
    "\n",
    "\n",
    "<b>Parallel Computing:</b>\n",
    "As we use loss at every node, we need not wait till the final output is availible for the model to train. The teachers forching method uses loss for every node output, they can be trained parallely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Problem 6:</strong></h1>\n",
    "\n",
    "<h2>Attention Is All You Need</h2>\n",
    "\n",
    "<h3><b>Introduction</b></h3>\n",
    "\n",
    "In sequence modeling and transduction challenges like language modeling and machine translation, recurrent neural networks, particularly long short-term memory and gated recurrent neural networks, have firmly established themselves as state-of-the-art techniques. The symbol positions of the input and output sequences are commonly factored in recurrent models. Sequential computation's fundamental limitation, on the other hand, persists. In different tasks, attention mechanisms have become an essential feature of appealing sequence modeling and transduction models, allowing for the modeling of dependencies regardless of their distance in the input or output sequences. However, except in a few circumstances, such attention techniques are always utilized in conjunction with a recurrent network. After only twelve hours of training on eight P100 GPUs, the Transformer allows for substantially higher parallelization and can achieve a new state-of-the-art translation quality.\n",
    "\n",
    "<h3><b>Background</b></h3>\n",
    "\n",
    "The Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as a fundamental building block to compute hidden representations in parallel for all input and output points, to decrease sequential computation. In these models, the number of operations required to connect signals from two random input or output sites grows linearly for ConvS2S and logarithmically for ByteNet as the distance between them grows. This makes learning dependencies between distant sites more difficult. Self-attention, also known as intra-attention, is an attention mechanism that connects distinct points of a single sequence to compute a representation of it.\n",
    "\n",
    "End-to-end memory networks work well on simple-language question answering and language modeling tasks because they use a recurrent attention mechanism rather than sequence-aligned repetition. The transformer is the first transduction model to calculate its input and output representations using only self-attention rather than sequence-aligned RNNs or convolution.\n",
    "\n",
    "<h3><b>Model Architecture</b></h3>\n",
    "\n",
    "An encoder-decoder structure is used in most competitive neural sequence transduction models. The encoder converts a sequence of symbol representations (x 1,..., x n) into a sequence of continuous representations (z =). (z 1, ..., z n ). The model is auto-regressive at each phase, using previously created symbols as extra input to create the next. As depicted in the left and right portions of The Transformer -model architecture, the Transformer uses layered self-attention and point-wise, completely connected layers for both the encoder and decoder.\n",
    "\n",
    "\n",
    "<h3><b>Encoder and Decoder Stacks</b></h3>\n",
    "\n",
    "An encoder-decoder structure is used in most competitive neural sequence transduction models. The encoder converts a sequence of symbol representations (x 1,..., x n) into a sequence of continuous representations (z =). (z 1, ..., z n ). The model is auto-regressive at each phase, using previously created symbols as extra input to create the next. As depicted in the left and right portions of The Transformer -model architecture, the Transformer uses layered self-attention and point-wise, completely connected layers for both the encoder and decoder.\n",
    "\n",
    "\n",
    "<h3><b>Attention</b></h3>\n",
    "The mapping of a query and a set of key-value pairs to output can be characterized as an attention function, where the query, keys, values, and output are all vectors. The result is a weighted sum of the values, with the weight allocated to each value determined by the query's compatibility function with the relevant key.\n",
    "\n",
    "\n",
    "<h3><b>Scaled Dot-Product Attention</b></h3>\n",
    "In three ways, Transformer employs multi-head attention: • The queries in \"encoder-decoder attention\" levels come from the previous decoder layer, whereas the memory keys and values come from the encoder's output. This allows every decoder position to attend to overall input sequence positions. In sequence-to-sequence models, this mirrors the standard encoder-decoder attention methods. • Self-attention layers are included in the encoder. To keep the auto-regressive characteristic, we need to block leftward information flow in the decoder.\n",
    "\n",
    "\n",
    "<h3><b>Multi-Head Attention</b></h3>\n",
    "We discovered linearly projecting the queries, keys, and values h times with separate. Learned linear projections to d k, d k, and d v dimensions were more advantageous than conducting a single attention function with d model -dimensional keys, values, and queries. These are concatenated and projected again, yielding the final values. Averaging prevents this with a single attention head. We use h = 8 parallel attention layers or heads where the projections are parameter matrices. The total computational cost is comparable to single-head attention with full dimensions when we employ the lower dimension of each head.\n",
    "\n",
    "\n",
    "<h3><b>Applications of Attention in our Model</b></h3>\n",
    "In three ways, the Transformer employs multi-head attention: \n",
    "• The queries in \"encoder-decoder attention\" levels come from the previous decoder layer, whereas the memory keys and values come from the encoder's output. This allows every decoder position to attend to overall input sequence positions. In sequence-to-sequence models, this mirrors the standard encoder-decoder attention methods. • Self-attention layers are included in the encoder. To keep the auto-regressive characteristic, we need to block leftward information flow in the decoder.\n",
    "\n",
    "<h3><b>Position-wise Feed-Forward Networks</b></h3>\n",
    "An ultimately linked feed-forward network is applied to each position individually and identically in each layer of our encoder, decoder, and attention sub-layers. This is made up of two linear transformations separated by a ReLU activation. While the linear transformations are the same in all positions, the parameters of distinct layers are used. This can also be described as two convolutions with a kernel size of 1. The input and output layers have a dimensionality of d model = 512, while the inner layer has a dimensionality of df f = 2048.\n",
    "\n",
    "\n",
    "<h3><b>Embeddings and Softmax</b></h3>\n",
    "We use learnt embeddings to convert the input and output tokens to vectors of the dimension d model, just like other sequence transduction models. We employ the standard learned linear transformation and softmax function to convert the decoder output to projected next-token probabilities. Our model's two embedding layers and the pre-softmax linear transformation share the same weight matrix. We multiply those weights by d model in the embedding layers.\n",
    "\n",
    "<h3><b>Positional Encoding</b></h3>\n",
    "\n",
    "Because our model lacks recurrence and convolution, we must provide information about the relative or absolute positions of the tokens in the sequence for the model to use the order of the sequence. We append \"positional encodings\" to the input embeddings at the encoder and decoder stacks' bottoms. There are numerously learned and fixed positional encodings to choose from. The sine and cosine functions of various frequencies are used in this work, where pos is the position, and I is the dimension. From 2 through 10000 • 2, the wavelengths form a geometric progression. We chose this function because it would help the model learn to pay attention to relative positions fast. P E pos+k can be described as a linear function of P Epos for any fixed offset k.\n",
    "\n",
    "\n",
    "<h3><b>Why Self-Attention</b></h3>\n",
    "\n",
    "We compare various aspects of self-attention layers to recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1,..., x n) to another sequence of equal length (z 1,..., z n), with x I z I R d, such as a hidden layer in a typical sequence transduction encoder or decoder, in this section. We discuss three desiderata to motivate our use of self-attention. As a result, we compare the maximum path length between any two input and output places in networks made up of various layer types.\n",
    "\n",
    "Self-attention could be limited to only evaluating a neighborhood of size r in the input sequence centered around the relevant output position to increase computing performance for very long sequence applications. In future research, we intend to look into this approach further. However, even when k = n, the complexity of a separable convolution is the same as combining a self-attention layer with a point-wise feed-forward layer, as we do in our model.\n",
    "\n",
    "<h3><b>Training Data and Batching</b></h3>\n",
    "\n",
    "We used the WMT 2014 English-German dataset containing around 4.5 million sentence pairings. With a shared source-target vocabulary of around 37000 tokens, sentences were encoded using byte-pair encoding. We used the WMT 2014 English-French dataset, which had 36 million sentences and was separated into 32000 word-piece vocabulary for English-French. Sentence pairs were grouped based on the length of the sequence. Each training batch contains around 25000 source tokens and 25000 target tokens in a set of sentence pairs.\n",
    "\n",
    "<h3><b>Hardware and Schedule</b></h3>\n",
    "Our models were trained on a single system with eight NVIDIA P100 GPUs. Each training step took roughly 0.4 seconds for our base models utilizing the hyperparameters specified throughout the paper. The basis models were trained for 100,000 steps or 12 hours. The step time for our large models was 1.0 second. The huge models were put through 300,000 steps of training (3.5 days).\n",
    "\n",
    "<h3><b>Optimizer</b></h3>\n",
    "\n",
    "We used the Adam optimizer with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the learning rate throughout training according to the formula: This corresponds to increasing the learning rate linearly for the first warmup_steps training steps and decreasing it proportionally to the inverse square root of the step number.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Regularization</b></h3>\n",
    "\n",
    "During training, we use three different methods of regularization: Residual Dropout is a term that refers to a person who has We dropout each sub-output layer's before adding it to the sub-layer input and normalizing it. In both the encoder and decoder stacks, we also apply dropout to the sums of the embeddings and the positional encodings. We utilize a rate of P drop = 0.1 in the base model. Smoothing the labels We used label smoothing with a value of ls = 0.1 during training. Perplexity suffers when the model learns to be more unsure, while accuracy and the BLEU score improve.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Machine Translation</b></h3>\n",
    "\n",
    "The huge transformer model surpasses the best previously reported models (including ensembles) by more than 2.0 BLEU on the WMT 2014 English-to-German translation problem, setting a new state-of-the-art BLEU score of 28.4. Our huge model outperformed all previously published single models on the WMT 2014 English-to-French translation test, achieving a BLEU score of 41.0 at less than 1/4 the training cost of the previous state-of-the-art model on the WMT 2014 English-to-French translation job. We averaged the last 5 checkpoints written at 10-minute intervals to create a single model for the basic models. With a beam size of 4 and a length penalty of 0.6, we used a beam search. After testing on the development set, these hyperparameters were chosen.\n",
    "\n",
    "\n",
    "<h3><b>Model Variations</b></h3>\n",
    "\n",
    "We modified our base model in numerous ways to evaluate the importance of different Transformer components, measuring the change in performance on the development set, newstest2013, on English-to-German translation. As mentioned in the previous section, we employed beam search but no checkpoint averaging. While single-head attention is 0.9 BLEU less effective than the optimal option, quality suffers as the number of heads increases. Reducing the attention key size d k hurts model quality, as shown in Table 3 rows (B). This implies that determining compatibility is difficult and that a more comprehensive compatibility function than the dot product would be advantageous.\n",
    "\n",
    "\n",
    "<h3><b>English Constituency Parsing</b></h3>\n",
    "\n",
    "We tested the Transformer on English constituency parsing to see if it might be applied to other tasks. The output is subject to solid structural limitations and is substantially longer than the input, posing unique issues. Furthermore, RNN sequence-to-sequence models have not yet achieved state-of-the-art performances in small-data regimes. We raised the maximum output length to input length + 300 during inference. Despite the lack of task-specific tweaking, our findings reveal that our model outperforms all previous models except the Recurrent Neural Network Grammar, delivering better outcomes. The Transformer beats the Berkeley-Parser even when trained simply on the WSJ training set of 40K phrases, in contrast to RNN sequence-to-sequence models.\n",
    "\n",
    "\n",
    "\n",
    "<h3><b>Conclusion</b></h3>\n",
    "\n",
    "We developed the Transformer, the first sequence transduction model based purely on attention, in which multi-headed self-attention replaces the recurrent layers most typically used in encoder-decoder designs. We complete fresh WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks that are state-of-the-art. In the former challenge, our top model surpasses all previously published ensembles. We are optimistic about the future of attention-based models and intend to use them in various situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
